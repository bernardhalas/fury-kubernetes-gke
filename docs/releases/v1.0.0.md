# Release notes

This version contains a refactor of the [`gke-private` terraform
module](modules/gke-private) to improve its flexibility and functionalities.

### Breaking changes
- Removed `google_container_node_pool.main` resource
- Removed `google_container_node_pool.infra` resource

### Changes
- Switched from deprecated `region` attribute to `location` in `google_container_*` resources
- Updated variables names according to [Terraform naming
  convention](https://www.terraform.io/docs/extend/best-practices/naming.html)
- Improved handling of `master_authorized_networks_config` attribute
- Added `daily_maintenance_time` input variable
- Explicitely set `use_ip_aliases = true` in `ip_allocation_policy` block for
  future-proofing
- Explicitely set `provider = "CALICO"` in `network_policy` block otherwise the
  default was `PROVIDER_UNSPECIFIED`
- Improved `google_container_node_pool` resource creation handling
- Added taints handling in `google_container_node_pool` resources

### Migration
Rename the following input variables in your `module` stanzas.

| pre v1.x name            | v1.x name                |
| :-                       | :-                       |
| `kube-master-version`    | `kube_master_version`    |
| `bastion-count`          | `bastion_count`          |
| `bastion-machine-type`   | `bastion_machine_type`   |
| `bastion-ssh-enabled`    | `bastion_ssh_enabled`    |
| `bastion-vpn-enabled`    | `bastion_vpn_enabled`    |
| `kube-node-version`      | `kube_node_version`      |
| `master-authorized-cidr` | `master_authorized_cidr` |
| `subnetwork-master-cidr` | `subnetwork_master_cidr` |
| `subnetwork-node-cidr`   | `subnetwork_node_cidr`   |
| `subnetwork-pod-cidr`    | `subnetwork-pod-cidr`    |
| `subnetwork-svc-cidr`    | `subnetwork_svc_cidr`    |

Remove the following input variable from your `module` stanzas:
- `infra-node-number`
- `infra-node-os`
- `infra-node-type`
- `node-number`
- `node-os`
- `node-type`

Populate the new `node_pools` and `node_taints` input variables according to
your needs. For example, if you want to reproduce the pre-v1.x node pools, you
can add the following:
```hcl
node_pools = [
  {
    name         = "main"
    machine_type = "n1-standard-2"
  },
  {
    name         = "infra"
    machine_type = "n1-standard-2"
  },
]

node_taints = {
  main  = []
  infra = []
}
```

To update your existing GKE cluster
```bash
$ terraform plan -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_cluster.main
```
you should see that the Terraform plan wants just to update your existing
cluster and if you are fine with the plan
```bash
$ terraform apply -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_cluster.main
```

Than, to switch to the new node pools avoiding downtime, execute the following
```bash
$ terraform plan -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_node_pool.pool
```
you should see that the Terraform plan wants just to create new node pools and
if you are fine with the plan
```bash
$ terraform apply -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_node_pool.pool
```
In order to migrate workloads over to the node pools just created, you can
proceed to drain the old node pools with
```bash
$ kubectl drain --ignore-daemonsets --delete-local-data <NODE_NAME>
```
Finally, let Terraform delete the old node pools
```bash
$ terraform apply \
  -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_node_pool.main
  -target=module.<YOUR_GKE_PRIVATE_MODULE_NAME>.google_container_node_pool.infra
```
